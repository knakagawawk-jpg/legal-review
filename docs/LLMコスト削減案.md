# LLM呼び出しコスト削減案

## 現状分析

### 現在のコスト要因

1. **評価プロンプトが非常に長い** (310行)
   - 詳細な指示が含まれているが、一部は冗長な可能性がある
   - プロンプト全体が毎回送信される

2. **max_tokensが大きい**
   - 評価: `max_tokens=16384` (16Kトークン)
   - チャット: `max_tokens=4096` (4Kトークン)
   - 実際の出力がこれより少ない場合でも、上限が大きいとコストに影響

3. **入力テキストが長い**
   - 問題文、答案、出題趣旨、採点実感などが全て含まれる
   - 長い答案の場合、入力トークン数が非常に大きくなる

4. **キャッシュがない**
   - 同じ入力に対する結果を再利用する仕組みがない
   - 同じ問題・答案の再評価で毎回API呼び出しが発生

5. **チャット履歴の最適化がない**
   - チャット履歴が長くなりすぎる可能性がある
   - 古いメッセージも全て送信される

## コスト削減案

### 1. プロンプトの最適化（優先度: 高）

**効果**: 入力トークン数 10-20%削減

**実装内容**:
- 評価プロンプト (`prompts/main/evaluation.txt`) の冗長な部分を削減
- 重複する説明を統合
- 必須でない詳細な指示を簡潔化

**具体的な改善点**:
- Step 1-4の説明を簡潔化（現在は詳細すぎる）
- 評価のポイントセクションの重複を削減
- 出力形式の説明を簡潔化

### 2. キャッシュの実装（優先度: 高）

**効果**: 同じ入力に対する再評価で 100%削減（初回のみコスト）

**実装内容**:
- 入力テキスト（問題文、答案、出題趣旨）のハッシュを計算
- 同じハッシュの結果をデータベースに保存・再利用
- キャッシュの有効期限を設定（例: 30日）

**実装場所**:
- `app/llm_service.py` の `generate_review` 関数
- 新しいテーブル: `review_cache` (hash, review_json, created_at)

**注意点**:
- プロンプトのバージョン変更時はキャッシュを無効化
- ユーザーが明示的に再評価を要求した場合はキャッシュをスキップ

### 3. max_tokensの最適化（優先度: 中）

**効果**: 出力トークン数の上限を適切に設定

**実装内容**:
- 評価: `16384` → `8192` に削減（実際の出力は通常これより少ない）
- チャット: `4096` → `2048` に削減（会話は通常これより短い）
- タイトル生成: 既に `50` で適切

**注意点**:
- 実際の使用量をモニタリングして調整
- 出力が途中で切れる場合は上限を増やす

### 4. 入力テキストの要約（優先度: 中）

**効果**: 非常に長い答案の場合、入力トークン数 30-50%削減

**実装内容**:
- 答案が一定の長さ（例: 5000文字）を超える場合、要約を生成
- 要約は低コストモデル（Haiku）で生成
- 要約と元の答案の一部を組み合わせて評価

**実装場所**:
- `app/llm_service.py` に `_summarize_long_answer` 関数を追加
- `generate_review` 関数で長い答案を検出して要約

**注意点**:
- 要約の品質を保つ必要がある
- 重要な部分は必ず含める

### 5. チャット履歴の最適化（優先度: 中）

**効果**: 長いチャットで入力トークン数 20-40%削減

**実装内容**:
- チャット履歴の長さを制限（例: 最新10件のみ）
- または、古いメッセージを要約して送信
- システムプロンプトで「最新の会話のみ参照」と指示

**実装場所**:
- `chat_about_review` 関数
- `free_chat` 関数

### 6. temperatureの調整（優先度: 低）

**効果**: 出力の一貫性向上、再試行の削減

**実装内容**:
- 評価: `0.3` → `0.2` に削減（より一貫した出力）
- チャット: `0.7` → `0.5` に削減（バランス型）

**注意点**:
- 出力の多様性が失われる可能性がある
- 実際の使用感を確認して調整

### 7. ストリーミングの活用（優先度: 低）

**効果**: ユーザー体験向上（コスト削減には直接影響しない）

**実装内容**:
- ストリーミングレスポンスでユーザーに早期フィードバック
- タイムアウトエラーの削減

## 実装優先順位

1. **最優先**: プロンプトの最適化 + キャッシュの実装
   - 効果が大きく、実装が比較的容易

2. **次優先**: max_tokensの最適化 + チャット履歴の最適化
   - 効果が中程度、実装が容易

3. **検討**: 入力テキストの要約
   - 効果は大きいが、実装が複雑で品質への影響を確認する必要がある

## 期待される効果

### 最優先実装（プロンプト最適化 + キャッシュ）の場合

- **初回評価**: 入力トークン数 10-20%削減
- **再評価**: 100%削減（キャッシュヒット時）
- **全体**: 使用量の30-50%削減（キャッシュヒット率による）

### 全実装の場合

- **初回評価**: 入力トークン数 20-40%削減
- **出力トークン数**: 10-20%削減（max_tokens最適化）
- **再評価**: 100%削減（キャッシュヒット時）
- **全体**: 使用量の50-70%削減

## 実装時の注意点

1. **品質の維持**: コスト削減のために品質を下げない
2. **モニタリング**: 実装後もトークン使用量を継続的に監視
3. **段階的実装**: 一度に全てを実装せず、段階的に実装して効果を測定
4. **ユーザー体験**: キャッシュの使用をユーザーに明示（オプション）

## 参考: Anthropic APIの料金（2025年1月時点）

- **Claude Haiku 4.5**: 入力 $0.25/1M tokens, 出力 $1.25/1M tokens
- **Claude Sonnet 3.5**: 入力 $3/1M tokens, 出力 $15/1M tokens
- **Claude Opus**: 入力 $15/1M tokens, 出力 $75/1M tokens

現在のデフォルトは Haiku 4.5 のため、コストは比較的低いが、使用量が多い場合は削減効果が大きい。
