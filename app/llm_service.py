import os
import re
import json
import sys
from pathlib import Path
from typing import Dict, Any, Optional, List
from anthropic import Anthropic

# è¨­å®šã‚’èª­ã¿è¾¼ã‚€ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ç’°å¢ƒå¤‰æ•°ã‚‚ç¢ºèªï¼‰
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
try:
    from config.llm_config import (
        get_llm_config, get_llm_client, get_llm_model,
        USE_CASE_REVIEW, USE_CASE_REVIEW_CHAT, USE_CASE_FREE_CHAT,
        USE_CASE_REVISIT_PROBLEMS, USE_CASE_TITLE
    )
except ImportError:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ—§æ–¹å¼ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
    try:
        from config.settings import ANTHROPIC_API_KEY, ANTHROPIC_MODEL
    except ImportError:
        ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
        ANTHROPIC_MODEL = os.getenv("ANTHROPIC_MODEL", "claude-haiku-4-5-20251001")
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ç°¡æ˜“é–¢æ•°
    def get_llm_config():
        class DummyConfig:
            def is_available(self):
                return bool(ANTHROPIC_API_KEY)
            def get_client(self):
                return Anthropic(api_key=ANTHROPIC_API_KEY) if ANTHROPIC_API_KEY else None
            def get_model(self, use_case="default"):
                return ANTHROPIC_MODEL
        return DummyConfig()
    
    def get_llm_client():
        if not ANTHROPIC_API_KEY:
            return None
        return Anthropic(api_key=ANTHROPIC_API_KEY)
    
    def get_llm_model(use_case="default"):
        return ANTHROPIC_MODEL
    
    USE_CASE_REVIEW = "review"
    USE_CASE_REVIEW_CHAT = "review_chat"
    USE_CASE_FREE_CHAT = "free_chat"
    USE_CASE_REVISIT_PROBLEMS = "revisit_problems"
    USE_CASE_TITLE = "title"

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
PROMPTS_DIR = Path(__file__).parent.parent / "prompts"

# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼ˆæ—¢å­˜ã‚³ãƒ¼ãƒ‰ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ï¼‰
_llm_config = get_llm_config()
ANTHROPIC_API_KEY = None  # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ä¸Šã®ç†ç”±ã§å…¬é–‹ã—ãªã„
ANTHROPIC_MODEL = _llm_config.get_model()  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«

def generate_review(
    subject: str,
    question_text: Optional[str],
    answer_text: str,
    purpose_text: Optional[str] = None,
    grading_impression_text: Optional[str] = None,
) -> tuple[str, Dict[str, Any], str, Optional[int], Optional[int], Optional[str], Optional[int]]:
    """
    LLMã‚’ä½¿ã£ã¦ç­”æ¡ˆã®è¬›è©•ã‚’ç”Ÿæˆã™ã‚‹ï¼ˆ1æ®µéšå‡¦ç†ï¼šç­”æ¡ˆã‚’ç›´æ¥è©•ä¾¡ï¼‰
    
    Returns:
        tuple: (review_markdown, review_json, model_name, input_tokens, output_tokens, request_id, latency_ms)
    """
    # LLMè¨­å®šã‚’å–å¾—
    llm_config = get_llm_config()
    
    # APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒ€ãƒŸãƒ¼ã‚’è¿”ã™
    if not llm_config.is_available():
        review_markdown, review_json, model_name = _generate_dummy_review(subject)
        return review_markdown, review_json, model_name, None, None, None, None
    
    try:
        client = llm_config.get_client()
        model_name = llm_config.get_model(USE_CASE_REVIEW)
        
        # ===== ç­”æ¡ˆã®è©•ä¾¡ï¼ˆ1æ®µéšå‡¦ç†ï¼‰ =====
        print("ç­”æ¡ˆã®è©•ä¾¡ã‚’é–‹å§‹...")
        evaluation_result, usage = _evaluate_answer(
            client,
            subject,
            question_text,
            answer_text,
            purpose_text,
            grading_impression_text,
            model_name,
        )
        
        # ===== è¬›è©•JSONã‚’æ§‹ç¯‰ï¼ˆevaluationã®ã¿ä¿æŒï¼‰ =====
        review_json = {
            "evaluation": evaluation_result
        }
        
        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã®è¬›è©•ã‚’ç”Ÿæˆ
        review_markdown = _format_markdown(subject, review_json)
        
        return (
            review_markdown,
            review_json,
            model_name,
            usage.get("input_tokens"),
            usage.get("output_tokens"),
            usage.get("request_id"),
            usage.get("latency_ms"),
        )
        
    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ä¾‹å¤–ã‚’å†ç™ºç”Ÿã•ã›ã‚‹ï¼ˆFastAPIã§å‡¦ç†ï¼‰
        import traceback
        error_detail = traceback.format_exc()
        print(f"LLMç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}\n{error_detail}")
        raise Exception(f"è¬›è©•ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}") from e


# _jsonize_answeré–¢æ•°ã¯å‰Šé™¤ï¼ˆ1æ®µéšå‡¦ç†ã§ã¯ä¸è¦ï¼‰

PARAGRAPH_MARKER_PREFIX = "$$["  # ç­”æ¡ˆã«ã¯å‡ºã¦ã“ãªã„å½¢å¼ã€‚è¡¨ç¤ºæ™‚ã« Â¶N ç­‰ã«å¤‰æ›ã™ã‚‹ã€‚


def add_paragraph_markers(answer_text: str) -> str:
    """
    ç­”æ¡ˆã‚’æ”¹è¡ŒåŒºåˆ‡ã‚Šï¼ˆéç©ºè¡Œã”ã¨ï¼‰ã§åˆ†å‰²ã—ã€å„éç©ºè¡Œã®è¡Œé ­ã« $$[1], $$[2], ... ã‚’ä»˜ä¸ã™ã‚‹ã€‚
    ç©ºè¡Œã¯ãã®ã¾ã¾ä¿æŒã™ã‚‹ã€‚æ®µè½ç•ªå·ã¯ç©ºè¡Œã‚’ã‚«ã‚¦ãƒ³ãƒˆã—ãªã„ã€‚
    """
    if not answer_text:
        return answer_text
    lines = answer_text.split("\n")
    para_num = 0
    result_lines = []
    for line in lines:
        if line.strip():
            para_num += 1
            result_lines.append(f"{PARAGRAPH_MARKER_PREFIX}{para_num}] {line}")
        else:
            result_lines.append(line)
    return "\n".join(result_lines)


def _evaluate_answer(
    client: Anthropic,
    subject: str,
    question_text: Optional[str],
    answer_text: str,
    purpose_text: Optional[str] = None,
    grading_impression_text: Optional[str] = None,
    model_name: Optional[str] = None,
) -> tuple[Dict[str, Any], Dict[str, Any]]:
    """ç­”æ¡ˆã‚’ç›´æ¥è©•ä¾¡ï¼ˆevaluation.txtä½¿ç”¨ï¼‰"""
    import time
    try:
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã‚€
        template = _load_prompt_template("evaluation")
        
        if not template:
            # è©•ä¾¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒãªã„å ´åˆã¯ç©ºã®è©•ä¾¡çµæœã‚’è¿”ã™
            return ({
                "overall_review": {
                    "score": 65,
                    "comment": "è©•ä¾¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ"
                },
                "strengths": [],
                "weaknesses": [],
                "future_considerations": []
            }, {"input_tokens": None, "output_tokens": None, "request_id": None, "latency_ms": None})
        
        # ç§‘ç›®åˆ¥ã®ç•™æ„äº‹é …ã‚’èª­ã¿è¾¼ã‚€
        subject_guidelines = _load_subject_guidelines(subject)
        
        # ç­”æ¡ˆã«æ®µè½ç•ªå·ãŒæ—¢ã«ä»˜ä¸ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        import re
        has_markers = bool(re.search(r'\$\$\[\d+\]', answer_text))
        
        # æ®µè½ç•ªå·ãŒã¾ã ä»˜ä¸ã•ã‚Œã¦ã„ãªã„å ´åˆã®ã¿ä»˜ä¸
        if has_markers:
            marked_answer = answer_text
        else:
            marked_answer = add_paragraph_markers(answer_text)
        
        prompt = template.replace("{SUBJECT_SPECIFIC_GUIDELINES}", subject_guidelines)
        prompt = prompt.replace("{PURPOSE_TEXT}", purpose_text or "ï¼ˆå‡ºé¡Œè¶£æ—¨ãªã—ï¼‰")
        prompt = prompt.replace("{GRADING_IMPRESSION_TEXT}", grading_impression_text or "ï¼ˆæ¡ç‚¹å®Ÿæ„Ÿãªã—ï¼‰")
        prompt = prompt.replace("{QUESTION_TEXT}", question_text or "ï¼ˆå•é¡Œæ–‡ãªã—ï¼‰")
        prompt = prompt.replace("{ANSWER_TEXT}", marked_answer)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰
        system_prompt = "ã‚ãªãŸã¯å¸æ³•è©¦é¨“ãƒ»äºˆå‚™è©¦é¨“ã®æ³•å¾‹ç­”æ¡ˆè¬›è©•ã®å“è³ªã‚’è©•ä¾¡ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚"
        
        # ãƒ¢ãƒ‡ãƒ«åã‚’å–å¾—ï¼ˆå¼•æ•°ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        if model_name is None:
            model_name = get_llm_model(USE_CASE_REVIEW)
        
        # Claude APIã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        start_time = time.time()
        message = client.messages.create(
            model=model_name,
            max_tokens=16384,  # é•·ã„è©•ä¾¡ã«å¯¾å¿œã™ã‚‹ãŸã‚ã•ã‚‰ã«å¢—åŠ ï¼ˆ16Kãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
            temperature=0.3,
            system=system_prompt,
            messages=[
                {
                    "role": "user",
                    "content": prompt + "\n\né‡è¦: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¯å¿…ãšæœ‰åŠ¹ãªJSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ã€‚æ–‡å­—åˆ—å†…ã®æ”¹è¡Œã‚„ç‰¹æ®Šæ–‡å­—ã¯é©åˆ‡ã«ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ã¦ãã ã•ã„ã€‚"
                }
            ]
        )
        latency_ms = int((time.time() - start_time) * 1000)
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
        if not message.content or len(message.content) == 0:
            raise Exception("LLMã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒç©ºã§ã™")
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
        usage = {
            "input_tokens": None,
            "output_tokens": None,
            "request_id": getattr(message, "id", None),
            "latency_ms": latency_ms,
        }
        if hasattr(message, 'usage') and message.usage:
            import logging
            logger = logging.getLogger(__name__)
            input_tokens = getattr(message.usage, 'input_tokens', 0)
            output_tokens = getattr(message.usage, 'output_tokens', 0)
            total_tokens = input_tokens + output_tokens
            logger.info(f"è©•ä¾¡ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡: å…¥åŠ›={input_tokens}, å‡ºåŠ›={output_tokens}, åˆè¨ˆ={total_tokens}")
            print(f"è©•ä¾¡ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡: å…¥åŠ›={input_tokens}, å‡ºåŠ›={output_tokens}, åˆè¨ˆ={total_tokens}")
            usage["input_tokens"] = input_tokens
            usage["output_tokens"] = output_tokens
        
        content = message.content[0].text
        content = _extract_json_from_response(content)
        
        # JSONã®ä¿®å¾©ã‚’è©¦ã¿ã‚‹ï¼ˆç°¡å˜ãªä¿®å¾©ã®ã¿ï¼‰
        content = _try_repair_json(content)
        
        try:
            return json.loads(content), usage
        except json.JSONDecodeError as e:
            # ã‚¨ãƒ©ãƒ¼ä½ç½®ã®å‰å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            error_pos = getattr(e, 'pos', None)
            if error_pos:
                start = max(0, error_pos - 200)
                end = min(len(content), error_pos + 200)
                error_context = content[start:end]
                error_marker = " " * (error_pos - start) + "^" if error_pos >= start else ""
                error_detail = f"ã‚¨ãƒ©ãƒ¼ä½ç½®: {error_pos}æ–‡å­—ç›® (è¡Œ{e.lineno}, åˆ—{e.colno})\nã‚¨ãƒ©ãƒ¼å‰å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆ:\n{error_context}\n{error_marker}"
            else:
                error_detail = f"ã‚¨ãƒ©ãƒ¼: {str(e)}"
            
            # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«å®Œå…¨ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¿å­˜
            import logging
            logger = logging.getLogger(__name__)
            logger.error(f"JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ï¼ˆè©•ä¾¡ï¼‰: {error_detail}\nå®Œå…¨ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆæœ€åˆã®2000æ–‡å­—ï¼‰: {content[:2000]}")
            
            raise Exception(f"JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ï¼ˆè©•ä¾¡ï¼‰: {str(e)}\n{error_detail}\nãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®æœ€åˆã®500æ–‡å­—: {content[:500]}")
    except Exception as e:
        error_type = type(e).__name__
        raise Exception(f"ç­”æ¡ˆã®è©•ä¾¡ã«å¤±æ•—ã—ã¾ã—ãŸ [{error_type}]: {str(e)}") from e


def _try_repair_json(json_str: str) -> str:
    """JSONã®ç°¡å˜ãªä¿®å¾©ã‚’è©¦ã¿ã‚‹"""
    import re
    
    # 1. æ–‡å­—åˆ—ãƒªãƒ†ãƒ©ãƒ«å†…ã®æ”¹è¡Œã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
    # ãƒ€ãƒ–ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆã§å›²ã¾ã‚ŒãŸéƒ¨åˆ†ã‚’æ¢ã™
    result = []
    i = 0
    in_string = False
    escape_next = False
    string_start = -1
    
    while i < len(json_str):
        char = json_str[i]
        
        if escape_next:
            result.append(char)
            escape_next = False
        elif char == '\\':
            result.append(char)
            escape_next = True
        elif char == '"' and not escape_next:
            if not in_string:
                # æ–‡å­—åˆ—ã®é–‹å§‹
                in_string = True
                string_start = len(result)
            else:
                # æ–‡å­—åˆ—ã®çµ‚äº†
                in_string = False
                string_start = -1
            result.append(char)
        elif in_string:
            if char == '\n':
                result.append('\\n')
            elif char == '\r':
                result.append('\\r')
            elif char == '\t':
                result.append('\\t')
            elif char == '"':
                result.append('\\"')
            else:
                result.append(char)
        else:
            result.append(char)
        
        i += 1
    
    json_str = ''.join(result)
    
    # 2. æœªçµ‚äº†ã®æ–‡å­—åˆ—ãƒªãƒ†ãƒ©ãƒ«ã‚’ä¿®å¾©ï¼ˆæ–‡å­—åˆ—ãŒé–‰ã˜ã‚‰ã‚Œã¦ã„ãªã„å ´åˆï¼‰
    # æ–‡å­—åˆ—ã®é–‹å§‹ä½ç½®ã‚’è¿½è·¡ã—ã€é–‰ã˜ã‚‰ã‚Œã¦ã„ãªã„æ–‡å­—åˆ—ã‚’é–‰ã˜ã‚‹
    result = []
    i = 0
    in_string = False
    escape_next = False
    brace_count = 0
    bracket_count = 0
    
    while i < len(json_str):
        char = json_str[i]
        
        if escape_next:
            result.append(char)
            escape_next = False
        elif char == '\\':
            result.append(char)
            escape_next = True
        elif char == '"' and not escape_next:
            in_string = not in_string
            result.append(char)
        elif not in_string:
            if char == '{':
                brace_count += 1
            elif char == '}':
                brace_count -= 1
            elif char == '[':
                bracket_count += 1
            elif char == ']':
                bracket_count -= 1
            result.append(char)
        else:
            result.append(char)
        
        i += 1
    
    json_str = ''.join(result)
    
    # æœªçµ‚äº†ã®æ–‡å­—åˆ—ãŒã‚ã‚Œã°é–‰ã˜ã‚‹ï¼ˆæœ€å¾Œã®æ–‡å­—åˆ—ãŒé–‰ã˜ã‚‰ã‚Œã¦ã„ãªã„å ´åˆï¼‰
    if in_string:
        # æ–‡å­—åˆ—ã‚’é–‰ã˜ã‚‹å‰ã«ã€æ”¹è¡Œã‚„ç‰¹æ®Šæ–‡å­—ã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
        json_str += '"'
        # ãã®å¾Œã€æœªé–‰ã˜ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ/é…åˆ—ã‚’é–‰ã˜ã‚‹å¿…è¦ãŒã‚ã‚‹
        # ãŸã ã—ã€æ–‡å­—åˆ—å†…ã«ã„ãŸå ´åˆã¯ã€ãã®å‰ã®æ§‹é€ ã‚‚é–‰ã˜ã‚‹å¿…è¦ãŒã‚ã‚‹
        # ç°¡æ˜“çš„ãªä¿®å¾©: æ–‡å­—åˆ—ã‚’é–‰ã˜ãŸå¾Œã€ã‚«ãƒ³ãƒã‚’è¿½åŠ ã—ã¦ã‹ã‚‰é–‰ã˜ã‚‹
        if brace_count > 0 or bracket_count > 0:
            # æ–‡å­—åˆ—ã®å¾Œã«ã‚«ãƒ³ãƒã‚’è¿½åŠ ï¼ˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ/é…åˆ—å†…ã®å ´åˆï¼‰
            if json_str.rstrip().endswith('"') and not json_str.rstrip().endswith('",'):
                # æœ€å¾Œã®æ–‡å­—åˆ—ã®å¾Œã«ã‚«ãƒ³ãƒãŒãªã„å ´åˆã€è¿½åŠ ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹åˆ¤æ–­
                # ãŸã ã—ã€ã“ã‚Œã¯è¤‡é›‘ãªã®ã§ã€å˜ç´”ã«é–‰ã˜ã‚‹
                pass
    
    # 3. æ‹¬å¼§ã§å›²ã¾ã‚ŒãŸæ•°å€¤ã‚’é€šå¸¸ã®æ•°å€¤ã«å¤‰æ›ï¼ˆä¾‹: (1) â†’ 1ï¼‰
    # æ–‡å­—åˆ—å¤–ã§ã®ã¿ç½®æ›ã™ã‚‹ãŸã‚ã€æ–‡å­—åˆ—å†…ã®æ‹¬å¼§ã¯é™¤å¤–
    def replace_parenthesized_numbers(text):
        result = []
        i = 0
        in_string = False
        escape_next = False
        
        while i < len(text):
            char = text[i]
            
            if escape_next:
                result.append(char)
                escape_next = False
            elif char == '\\':
                result.append(char)
                escape_next = True
            elif char == '"' and not escape_next:
                in_string = not in_string
                result.append(char)
            elif not in_string:
                # æ–‡å­—åˆ—å¤–ã§æ‹¬å¼§ã§å›²ã¾ã‚ŒãŸæ•°å€¤ã‚’æ¤œå‡º
                if char == '(':
                    # æ¬¡ã®æ–‡å­—ãŒæ•°å­—ã‹ç¢ºèª
                    num_start = i + 1
                    num_end = num_start
                    while num_end < len(text) and text[num_end].isdigit():
                        num_end += 1
                    # é–‰ã˜æ‹¬å¼§ãŒã‚ã‚‹ã‹ç¢ºèª
                    if num_end < len(text) and text[num_end] == ')':
                        # æ•°å€¤ã‚’æŠ½å‡ºã—ã¦æ‹¬å¼§ãªã—ã§è¿½åŠ 
                        number = text[num_start:num_end]
                        result.append(number)
                        i = num_end  # é–‰ã˜æ‹¬å¼§ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    else:
                        result.append(char)
                else:
                    result.append(char)
            else:
                result.append(char)
            
            i += 1
        
        return ''.join(result)
    
    json_str = replace_parenthesized_numbers(json_str)
    
    # 4. æœ«å°¾ã®ä¸è¦ãªã‚«ãƒ³ãƒã‚’å‰Šé™¤ï¼ˆãŸã ã—ã€æ–‡å­—åˆ—å†…ã®ã‚«ãƒ³ãƒã¯é™¤å¤–ï¼‰
    json_str = re.sub(r',(\s*[}\]])', r'\1', json_str)
    
    # 5. æœªé–‰ã˜ã®æ‹¬å¼§ã‚’é–‰ã˜ã‚‹
    while brace_count > 0:
        json_str += '}'
        brace_count -= 1
    while bracket_count > 0:
        json_str += ']'
        bracket_count -= 1
    
    return json_str


def _extract_json_from_response(content: str) -> str:
    """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰JSONã‚’æŠ½å‡ºï¼ˆã‚ˆã‚Šå …ç‰¢ãªæ–¹æ³•ï¼‰"""
    import re
    
    original_content = content
    content = content.strip()
    
    # ```json ... ``` ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™ï¼ˆobject/arrayä¸¡å¯¾å¿œï¼‰
    json_block_pattern = r'```(?:json)?\s*([\s\S]*?)\s*```'
    match = re.search(json_block_pattern, content, re.DOTALL)
    if match:
        extracted = match.group(1).strip()
        # å…ˆé ­ã®JSONé–‹å§‹è¨˜å·ã¾ã§åˆ‡ã‚Šè©°ã‚
        brace = extracted.find("{")
        bracket = extracted.find("[")
        starts = [x for x in [brace, bracket] if x != -1]
        if starts:
            extracted = extracted[min(starts):].strip()
        return extracted
    
    # ``` ... ``` ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™ï¼ˆjsonæŒ‡å®šãªã—ï¼‰
    code_block_pattern = r'```\s*([\s\S]*?)\s*```'
    match = re.search(code_block_pattern, content, re.DOTALL)
    if match:
        extracted = match.group(1).strip()
        brace = extracted.find("{")
        bracket = extracted.find("[")
        starts = [x for x in [brace, bracket] if x != -1]
        if starts:
            extracted = extracted[min(starts):].strip()
        return extracted
    
    # { ã¾ãŸã¯ [ ã‹ã‚‰å§‹ã¾ã‚‹JSONã‚’æ¢ã™ï¼ˆãƒã‚¹ãƒˆè€ƒæ…®ï¼‰
    brace_start = content.find("{")
    bracket_start = content.find("[")
    starts = [(brace_start, "{"), (bracket_start, "[")]
    starts = [(i, c) for (i, c) in starts if i != -1]
    if starts:
        json_start, start_char = min(starts, key=lambda x: x[0])
        brace_count = 0
        bracket_count = 0
        json_end = -1
        in_string = False
        escape_next = False

        for i in range(json_start, len(content)):
            char = content[i]

            if escape_next:
                escape_next = False
                continue
            if char == "\\":
                escape_next = True
                continue
            if char == '"' and not escape_next:
                in_string = not in_string
                continue

            if in_string:
                continue

            if char == "{":
                brace_count += 1
            elif char == "}":
                brace_count -= 1
            elif char == "[":
                bracket_count += 1
            elif char == "]":
                bracket_count -= 1

            # é–‹å§‹ãŒ { ã®å ´åˆã¯ brace ãŒ0ã«æˆ»ã£ãŸã‚‰çµ‚äº†
            if start_char == "{" and brace_count == 0 and i > json_start:
                json_end = i + 1
                break
            # é–‹å§‹ãŒ [ ã®å ´åˆã¯ bracket ãŒ0ã«æˆ»ã£ãŸã‚‰çµ‚äº†
            if start_char == "[" and bracket_count == 0 and i > json_start:
                json_end = i + 1
                break

        if json_end != -1:
            return content[json_start:json_end].strip()
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…ƒã®å‡¦ç†
    if content.startswith("```json"):
        content = content[7:]  # ```json ã‚’å‰Šé™¤
    if content.startswith("```"):
        content = content[3:]  # ``` ã‚’å‰Šé™¤
    if content.endswith("```"):
        content = content[:-3]  # ``` ã‚’å‰Šé™¤
    return content.strip()


def generate_recent_review_problems(
    prompt: str,
    max_tokens: int = 4096,
    temperature: float = 0.4,
) -> tuple[list[Dict[str, Any]], str, str, Optional[int], Optional[int], Optional[str], Optional[int]]:
    """
    æœ€è¿‘ã®å¾©ç¿’å•é¡Œã‚’ç”Ÿæˆï¼ˆJSONé…åˆ—ã‚’è¿”ã™ï¼‰

    Returns:
        (items, raw_output, model_name, input_tokens, output_tokens, request_id, latency_ms)
    """
    import time
    # LLMè¨­å®šã‚’å–å¾—
    llm_config = get_llm_config()
    
    # APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒ€ãƒŸãƒ¼
    if not llm_config.is_available():
        dummy = [
            {
                "subject_id": 1,
                "question_text": "ï¼ˆãƒ€ãƒŸãƒ¼ï¼‰é•æ†²å¯©æŸ»åŸºæº–ã®ä½¿ã„åˆ†ã‘ã‚’èª¬æ˜ã›ã‚ˆã€‚",
                "answer_example": "è¦åˆ¶ç›®çš„ãƒ»æ‰‹æ®µã®å¯©æŸ»å¯†åº¦ã«å¿œã˜ã¦åŸºæº–ã‚’ä½¿ã„åˆ†ã‘ã‚‹ã€‚",
                "references": "ã¾ãšæ¨©åˆ©ã®æ€§è³ªã¨è¦åˆ¶æ…‹æ§˜ã‚’ç‰¹å®šã—ã€ç›®çš„é‡è¦æ€§ã¨æ‰‹æ®µå¿…è¦æ€§ãƒ»åˆç†æ€§ã®è¦³ç‚¹ã§å¯©æŸ»å¯†åº¦ã‚’æ•´ç†ã™ã‚‹ã€‚",
            }
        ]
        return dummy, json.dumps(dummy, ensure_ascii=False), "dummy", None, None, None, None

    client = llm_config.get_client()
    model_name = llm_config.get_model(USE_CASE_REVISIT_PROBLEMS)

    system_prompt = (
        "ã‚ãªãŸã¯å¸æ³•è©¦é¨“ãƒ»äºˆå‚™è©¦é¨“ã®å­¦ç¿’æ”¯æ´è€…ã§ã™ã€‚"
        "ä¸ãˆã‚‰ã‚ŒãŸå­¦ç¿’å±¥æ­´ã«åŸºã¥ãã€å¾©ç¿’å•é¡Œã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"
        "å‡ºåŠ›ã¯å¿…ãšJSONã®ã¿ã§ã€ä½™è¨ˆãªæ–‡ç« ã‚’ä»˜ã‘ãªã„ã§ãã ã•ã„ã€‚"
    )

    start_time = time.time()
    message = client.messages.create(
        model=model_name,
        max_tokens=max_tokens,
        temperature=temperature,
        system=system_prompt,
        messages=[
            {
                "role": "user",
                "content": (prompt or "").strip()
                + "\n\né‡è¦: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¯å¿…ãšæœ‰åŠ¹ãªJSONé…åˆ—ã®ã¿ã§è¿”ã—ã¦ãã ã•ã„ã€‚æ–‡å­—åˆ—å†…ã®æ”¹è¡Œã‚„ç‰¹æ®Šæ–‡å­—ã¯é©åˆ‡ã«ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ã¦ãã ã•ã„ã€‚",
            }
        ],
    )

    raw_output = message.content[0].text if message.content and len(message.content) > 0 else ""
    latency_ms = int((time.time() - start_time) * 1000)

    input_tokens = None
    output_tokens = None
    if hasattr(message, "usage") and message.usage:
        input_tokens = getattr(message.usage, "input_tokens", None)
        output_tokens = getattr(message.usage, "output_tokens", None)
    request_id = getattr(message, "id", None)

    content = _extract_json_from_response(raw_output)
    content = _try_repair_json(content)

    data = json.loads(content)
    if isinstance(data, dict):
        # ãŸã¾ã« {"items": [...]} ã®å½¢ã§è¿”ã‚‹ã®ã‚’å¸å
        data = data.get("items", [])
    if not isinstance(data, list):
        raise Exception("LLM output is not a JSON array")

    items: list[Dict[str, Any]] = []
    for obj in data[:5]:
        if not isinstance(obj, dict):
            continue
        subject_id = obj.get("subject_id")
        if subject_id is not None:
            try:
                subject_id = int(subject_id)
            except Exception:
                subject_id = None
        if subject_id is not None and not (1 <= subject_id <= 18):
            subject_id = None
        question_text = str(obj.get("question_text") or "").strip()
        if not question_text:
            continue
        items.append(
            {
                "subject_id": subject_id,
                "question_text": question_text,
                "answer_example": (str(obj.get("answer_example") or "").strip() or None),
                "references": (str(obj.get("references") or "").strip() or None),
            }
        )

    return items, raw_output, model_name, input_tokens, output_tokens, request_id, latency_ms


# _build_final_reviewé–¢æ•°ã¯å‰Šé™¤ï¼ˆ1æ®µéšå‡¦ç†ã§ã¯ä¸è¦ï¼‰


def _load_prompt_template(template_name: str) -> str:
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    template_path = PROMPTS_DIR / "main" / f"{template_name}.txt"
    if not template_path.exists():
        raise FileNotFoundError(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {template_path}")
    return template_path.read_text(encoding="utf-8")

def _load_subject_guidelines(subject: str) -> str:
    """ç§‘ç›®åˆ¥ã®ç•™æ„äº‹é …ã‚’èª­ã¿è¾¼ã‚€"""
    # ç§‘ç›®åã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°
    subject_mapping = {
        "æ†² æ³•": "constitution",
        "æ†²æ³•": "constitution",
        "è¡Œæ”¿æ³•": "administrative_law",
        "æ°‘ æ³•": "civil_law",
        "æ°‘æ³•": "civil_law",
        "å•† æ³•": "commercial_law",
        "å•†æ³•": "commercial_law",
        "æ°‘äº‹è¨´è¨Ÿæ³•": "civil_procedure",
        "åˆ‘ æ³•": "criminal_law",
        "åˆ‘æ³•": "criminal_law",
        "åˆ‘äº‹è¨´è¨Ÿæ³•": "criminal_procedure",
        "å®Ÿå‹™åŸºç¤ï¼ˆæ°‘äº‹ï¼‰": "civil_practice",
        "å®Ÿå‹™åŸºç¤ï¼ˆåˆ‘äº‹ï¼‰": "criminal_practice",
    }
    
    file_name = subject_mapping.get(subject, "default")
    guidelines_path = PROMPTS_DIR / "subjects" / f"{file_name}.txt"
    
    if guidelines_path.exists():
        return guidelines_path.read_text(encoding="utf-8")
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        default_path = PROMPTS_DIR / "subjects" / "default.txt"
        if default_path.exists():
            return default_path.read_text(encoding="utf-8")
        return ""

# _build_prompté–¢æ•°ã¨_build_prompt_legacyé–¢æ•°ã¯å‰Šé™¤ï¼ˆ1æ®µéšå‡¦ç†ã§ã¯ä¸è¦ï¼‰

def _format_markdown(subject: str, review_json: Dict[str, Any]) -> str:
    """JSONå½¢å¼ã®è¬›è©•ã‚’ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã«å¤‰æ›ï¼ˆ1æ®µéšå‡¦ç†ç”¨ï¼ševaluationã®ã¿ï¼‰"""
    parts = [
        f"### ç­”æ¡ˆè¬›è©•",
        f"ç§‘ç›®: **{subject}**",
        f"",
    ]
    
    # æ–°ã—ã„æ§‹é€ ï¼ˆevaluationã®ã¿ï¼‰
    evaluation = review_json.get("evaluation", {})
    
    # ç·è©•ï¼ˆã‚¹ã‚³ã‚¢ã¨ã‚³ãƒ¡ãƒ³ãƒˆï¼‰
    overall_review = evaluation.get("overall_review", {})
    score = overall_review.get("score", 65)
    comment = overall_review.get("comment", "")
    
    parts.extend([
        f"### ğŸ“Š ç·è©•",
        f"",
        f"**ç‚¹æ•°: {score}ç‚¹**",
        f"",
    ])
    
    if comment:
        parts.extend([
            comment,
            "",
        ])
    
    # è©•ä¾¡ã—ãŸç‚¹ï¼ˆevaluationã‹ã‚‰ç›´æ¥å–å¾—ã—ã€æ®µè½ç•ªå·ãŒã‚ã‚‹å ´åˆã¯è¡¨ç¤ºï¼‰
    evaluation_strengths = evaluation.get("strengths", [])
    if evaluation_strengths:
        parts.extend([
            "### âœ… è©•ä¾¡ã—ãŸç‚¹",
            "",
        ])
        for strength in evaluation_strengths:
            if isinstance(strength, dict):
                category = strength.get("category", "")
                description = strength.get("description", "")
                para_nums = strength.get("paragraph_numbers", [])
                if category and description:
                    if para_nums:
                        parts.append(f"- **[{category}]** {description}ï¼ˆé–¢é€£æ®µè½: {', '.join(map(str, para_nums))}ï¼‰")
                    else:
                        parts.append(f"- **[{category}]** {description}")
                elif description:
                    if para_nums:
                        parts.append(f"- {description}ï¼ˆé–¢é€£æ®µè½: {', '.join(map(str, para_nums))}ï¼‰")
                    else:
                        parts.append(f"- {description}")
            elif isinstance(strength, str):
                parts.append(f"- {strength}")
        parts.append("")
    
    # æ”¹å–„ç‚¹ï¼ˆevaluationã‹ã‚‰ç›´æ¥å–å¾—ã—ã€æ®µè½ç•ªå·ãŒã‚ã‚‹å ´åˆã¯è¡¨ç¤ºï¼‰
    evaluation_weaknesses = evaluation.get("weaknesses", [])
    if evaluation_weaknesses:
        parts.extend([
            "### âš ï¸ æ”¹å–„ç‚¹",
            "",
        ])
        for weakness in evaluation_weaknesses:
            if isinstance(weakness, dict):
                category = weakness.get("category", "")
                description = weakness.get("description", "")
                para_nums = weakness.get("paragraph_numbers", [])
                suggestion = weakness.get("suggestion", "")
                if category and description:
                    if para_nums:
                        parts.append(f"- **[{category}]** {description}ï¼ˆé–¢é€£æ®µè½: {', '.join(map(str, para_nums))}ï¼‰")
                    else:
                        parts.append(f"- **[{category}]** {description}")
                    if suggestion:
                        parts.append(f"  - ğŸ’¡ æ”¹å–„ææ¡ˆ: {suggestion}")
                elif description:
                    if para_nums:
                        parts.append(f"- {description}ï¼ˆé–¢é€£æ®µè½: {', '.join(map(str, para_nums))}ï¼‰")
                    else:
                        parts.append(f"- {description}")
                    if suggestion:
                        parts.append(f"  - ğŸ’¡ æ”¹å–„ææ¡ˆ: {suggestion}")
            elif isinstance(weakness, str):
                parts.append(f"- {weakness}")
        parts.append("")
    
    # é‡è¦ãªãƒã‚¤ãƒ³ãƒˆãŒã‚ã‚‹å ´åˆï¼ˆevaluation.txtã®æ–°ã—ã„å‡ºåŠ›å½¢å¼ï¼‰
    important_points = evaluation.get("important_points", [])
    if important_points:
        parts.extend([
            "---",
            "### ğŸ“Œ é‡è¦ãªæ®µè½ã®è©•ä¾¡",
            "",
        ])
        
        for point in important_points:
            para_num = point.get("paragraph_number", "?")
            parts.extend([
                f"#### æ®µè½ {para_num}",
                "",
            ])
            
            if point.get("why_important"):
                parts.extend([
                    f"**é‡è¦æ€§**: {point['why_important']}",
                    "",
                ])
            
            if point.get("what_is_good"):
                parts.extend([
                    "**âœ… ååˆ†ã«æ›¸ã‘ã¦ã„ã‚‹ç‚¹:**",
                    point["what_is_good"],
                    "",
                ])
            
            if point.get("what_is_lacking"):
                parts.extend([
                    "**âš ï¸ ä¸è¶³ã—ã¦ã„ã‚‹ç‚¹:**",
                    point["what_is_lacking"],
                    "",
                ])
            
            parts.append("")
    
    # ãã®ä»–ä»Šå¾Œæ„è­˜ã™ã‚‹ã¹ãã“ã¨
    future_considerations = evaluation.get("future_considerations", [])
    if future_considerations:
        parts.extend([
            "### ğŸ“‹ ãã®ä»–ä»Šå¾Œæ„è­˜ã™ã‚‹ã¹ãã“ã¨",
            "",
        ])
        for action in future_considerations:
            parts.append(f"- {action}")
        parts.append("")
    
    # æ—§å½¢å¼ã®ã‚µãƒãƒ¼ãƒˆï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
    if not evaluation and review_json.get("score") is not None:
        parts.extend([
            f"**ç‚¹æ•°: {review_json.get('score', 'N/A')}ç‚¹**",
            f"",
        ])
        
        if review_json.get("strengths"):
            parts.extend([
                "### è‰¯ã„ç‚¹",
            ])
            for strength in review_json["strengths"]:
                parts.append(f"- {strength}")
            parts.append("")
        
        if review_json.get("weaknesses"):
            parts.extend([
                "### æ”¹å–„ç‚¹",
            ])
            for weakness in review_json["weaknesses"]:
                parts.append(f"- {weakness}")
            parts.append("")
        
        if review_json.get("next_actions"):
            parts.extend([
                "### æ¬¡ã«ã‚„ã‚‹ã“ã¨",
            ])
            for action in review_json["next_actions"]:
                parts.append(f"- {action}")
    
    return "\n".join(parts)


def _generate_dummy_review(subject: str) -> tuple[str, Dict[str, Any], str]:
    """ãƒ€ãƒŸãƒ¼è¬›è©•ã‚’ç”Ÿæˆï¼ˆAPIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆï¼‰"""
    review_json = {
        "score": 60,
        "strengths": ["è«–ç‚¹ã®æ‹¾ã„ä¸Šã’ã¯ã§ãã¦ã„ã‚‹"],
        "weaknesses": ["è¦ç¯„ã®å®šç«‹ãŒæ›–æ˜§", "ã‚ã¦ã¯ã‚ãŒè–„ã„"],
        "next_actions": ["è¦ç¯„ã‚’è¦ä»¶ã”ã¨ã«ç®‡æ¡æ›¸ã", "äº‹å®Ÿâ†’è©•ä¾¡èªâ†’çµè«–ã®æ¥ç¶šã‚’æ˜ç¤º"],
    }
    review_markdown = f"""
### ç·è©•
ç§‘ç›®: **{subject}**

**ç‚¹æ•°: 60ç‚¹**

### è‰¯ã„ç‚¹
- è«–ç‚¹ã®æ‹¾ã„ä¸Šã’ã¯ã§ãã¦ã„ã¾ã™

### æ”¹å–„ç‚¹
- è¦ç¯„ã®å®šç«‹ãŒæ›–æ˜§
- ã‚ã¦ã¯ã‚ãŒè–„ã„

### æ¬¡ã«ã‚„ã‚‹ã“ã¨
- è¦ç¯„ã‚’è¦ä»¶ã”ã¨ã«ç®‡æ¡æ›¸ã
- äº‹å®Ÿâ†’è©•ä¾¡èªâ†’çµè«–ã®æ¥ç¶šã‚’æ˜ç¤º
""".strip()
    
    return review_markdown, review_json, "dummy"


def chat_about_review(
    submission_id: int,
    question: str,
    question_text: str,
    answer_text: str,
    review_markdown: str,
    chat_history: Optional[List[Dict[str, str]]] = None
) -> tuple[str, str, Optional[int], Optional[int], Optional[str], Optional[int]]:
    """
    è¬›è©•ã«é–¢ã™ã‚‹è³ªå•ã«ç­”ãˆã‚‹
    
    Args:
        submission_id: æå‡ºID
        question: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
        question_text: å•é¡Œæ–‡
        answer_text: ç­”æ¡ˆ
        review_markdown: è¬›è©•ï¼ˆãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ï¼‰
        chat_history: ãƒãƒ£ãƒƒãƒˆå±¥æ­´ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    
    Returns:
        LLMã‹ã‚‰ã®å›ç­”
    """
    # LLMè¨­å®šã‚’å–å¾—
    llm_config = get_llm_config()
    
    # APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒ€ãƒŸãƒ¼ã‚’è¿”ã™
    if not llm_config.is_available():
        return (
            "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ç¾åœ¨LLMæ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ã‚’ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚",
            "dummy",
            None,
            None,
            None,
            None,
        )
    
    try:
        import time
        client = llm_config.get_client()
        model_name = llm_config.get_model(USE_CASE_REVIEW_CHAT)
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        system_prompt = """ã‚ãªãŸã¯æ³•å¾‹ç­”æ¡ˆã®è¬›è©•ã‚’è¡Œã†å°‚é–€å®¶ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã«å¯¾ã—ã¦ã€ä»¥ä¸‹ã®æƒ…å ±ã‚’å‚ç…§ã—ãªãŒã‚‰ä¸å¯§ã«å›ç­”ã—ã¦ãã ã•ã„ï¼š
- å•é¡Œæ–‡
- ç­”æ¡ˆ
- æ—¢ã«ç”Ÿæˆã•ã‚ŒãŸè¬›è©•

è¬›è©•ã®å†…å®¹ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ãŸã‚Šã€æ”¹å–„ç‚¹ã‚’å…·ä½“çš„ã«ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã—ãŸã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç–‘å•ã«ç­”ãˆã¦ãã ã•ã„ã€‚"""
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’æ§‹ç¯‰
        messages = []
        
        # æœ€åˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼šã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’æä¾›
        context_message = f"""ä»¥ä¸‹ã®å•é¡Œæ–‡ã€ç­”æ¡ˆã€è¬›è©•ã«ã¤ã„ã¦è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

ã€å•é¡Œæ–‡ã€‘
{question_text}

ã€ç­”æ¡ˆã€‘
{answer_text}

ã€è¬›è©•ã€‘
{review_markdown}

ä¸Šè¨˜ã®æƒ…å ±ã‚’å‚ç…§ã—ãªãŒã‚‰ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚"""
        
        messages.append({
            "role": "user",
            "content": context_message
        })
        
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’è¿½åŠ ï¼ˆæœ€åˆã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å¾Œï¼‰
        if chat_history:
            messages.extend(chat_history)
        
        # ç¾åœ¨ã®è³ªå•ã‚’è¿½åŠ 
        messages.append({
            "role": "user",
            "content": question
        })
        
        # Claude APIã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        start_time = time.time()
        message = client.messages.create(
            model=model_name,
            max_tokens=4096,
            temperature=0.7,
            system=system_prompt,
            messages=messages
        )
        latency_ms = int((time.time() - start_time) * 1000)
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—
        answer = message.content[0].text
        input_tokens = None
        output_tokens = None
        if hasattr(message, "usage") and message.usage:
            input_tokens = getattr(message.usage, "input_tokens", None)
            output_tokens = getattr(message.usage, "output_tokens", None)
        request_id = getattr(message, "id", None)
        return answer, model_name, input_tokens, output_tokens, request_id, latency_ms
        
    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
        print(f"ãƒãƒ£ãƒƒãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        model_name = llm_config.get_model(USE_CASE_REVIEW_CHAT)
        return (
            f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
            model_name,
            None,
            None,
            None,
            None,
        )


def free_chat(
    question: str,
    chat_history: Optional[List[Dict[str, str]]] = None
) -> tuple[str, str, Optional[int], Optional[int], Optional[str], Optional[int]]:
    """
    ãƒ•ãƒªãƒ¼ãƒãƒ£ãƒƒãƒˆï¼ˆæ–‡è„ˆã«ç¸›ã‚‰ã‚Œãªã„æ±ç”¨çš„ãªãƒãƒ£ãƒƒãƒˆï¼‰
    
    Args:
        question: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
        chat_history: ãƒãƒ£ãƒƒãƒˆå±¥æ­´ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    
    Returns:
        LLMã‹ã‚‰ã®å›ç­”
    """
    # LLMè¨­å®šã‚’å–å¾—
    llm_config = get_llm_config()
    
    # APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒ€ãƒŸãƒ¼ã‚’è¿”ã™
    if not llm_config.is_available():
        return (
            "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ç¾åœ¨LLMæ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ã‚’ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚",
            "dummy",
            None,
            None,
            None,
            None,
        )
    
    try:
        import time
        client = llm_config.get_client()
        model_name = llm_config.get_model(USE_CASE_FREE_CHAT)
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæ±ç”¨çš„ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¨ã—ã¦å‹•ä½œï¼‰
        system_prompt = """ã‚ãªãŸã¯è¦ªåˆ‡ã§çŸ¥è­˜è±Šå¯Œãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã«å¯¾ã—ã¦ã€æ­£ç¢ºã§åˆ†ã‹ã‚Šã‚„ã™ã„å›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
æ³•å¾‹ã«é–¢ã™ã‚‹è³ªå•ã«ã¤ã„ã¦ã¯ã€å¯èƒ½ãªç¯„å›²ã§æ³•çš„ãªè¦³ç‚¹ã‹ã‚‰èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
ä¸€èˆ¬çš„ãªè³ªå•ã«ã¤ã„ã¦ã‚‚ã€ä¸å¯§ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"""
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’æ§‹ç¯‰
        messages = []
        
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ãŒã‚ã‚Œã°è¿½åŠ 
        if chat_history:
            messages.extend(chat_history)
        
        # ç¾åœ¨ã®è³ªå•ã‚’è¿½åŠ 
        messages.append({
            "role": "user",
            "content": question
        })
        
        # Claude APIã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        start_time = time.time()
        message = client.messages.create(
            model=model_name,
            max_tokens=4096,
            temperature=0.7,
            system=system_prompt,
            messages=messages
        )
        latency_ms = int((time.time() - start_time) * 1000)
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—
        answer = message.content[0].text
        input_tokens = None
        output_tokens = None
        if hasattr(message, "usage") and message.usage:
            input_tokens = getattr(message.usage, "input_tokens", None)
            output_tokens = getattr(message.usage, "output_tokens", None)
        request_id = getattr(message, "id", None)
        return answer, model_name, input_tokens, output_tokens, request_id, latency_ms
        
    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
        print(f"ãƒ•ãƒªãƒ¼ãƒãƒ£ãƒƒãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        model_name = llm_config.get_model(USE_CASE_FREE_CHAT)
        return (
            f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
            model_name,
            None,
            None,
            None,
            None,
        )


def generate_chat_title(
    first_message: str,
    max_tokens: int = 50,
) -> tuple[str, str, Optional[int], Optional[int], Optional[str], Optional[int]]:
    """
    ãƒãƒ£ãƒƒãƒˆã®æœ€åˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹
    
    Args:
        first_message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æœ€åˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆçŸ­ã„ã‚¿ã‚¤ãƒˆãƒ«ç”¨ï¼‰
    
    Returns:
        tuple: (title, model_name, input_tokens, output_tokens, request_id, latency_ms)
    """
    import time
    # LLMè¨­å®šã‚’å–å¾—
    llm_config = get_llm_config()
    
    if not llm_config.is_available():
        # APIã‚­ãƒ¼ãŒãªã„å ´åˆã¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å…ˆé ­ã‚’è¿”ã™
        title = first_message[:20] + "..." if len(first_message) > 20 else first_message
        return title, "dummy", None, None, None, None
    
    try:
        client = llm_config.get_client()
        model_name = llm_config.get_model(USE_CASE_TITLE)
        
        system_prompt = """ã‚ãªãŸã¯ãƒãƒ£ãƒƒãƒˆã®ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ã€ãã®ãƒãƒ£ãƒƒãƒˆã®å†…å®¹ã‚’ç«¯çš„ã«è¡¨ã™çŸ­ã„ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆ15æ–‡å­—ä»¥å†…ï¼‰ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿ã‚’å‡ºåŠ›ã—ã€ä»–ã®èª¬æ˜ã¯ä¸è¦ã§ã™ã€‚"""
        
        start_time = time.time()
        message = client.messages.create(
            model=model_name,
            max_tokens=max_tokens,
            temperature=0.3,
            system=system_prompt,
            messages=[{
                "role": "user",
                "content": f"ä»¥ä¸‹ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å¯¾ã—ã¦ã€15æ–‡å­—ä»¥å†…ã®çŸ­ã„ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š\n\n{first_message}"
            }]
        )
        latency_ms = int((time.time() - start_time) * 1000)
        
        title = message.content[0].text.strip() if message.content else ""
        # ã‚¿ã‚¤ãƒˆãƒ«ãŒé•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚ã‚‹
        if len(title) > 20:
            title = title[:20]
        title = title if title else first_message[:20]
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’å–å¾—
        input_tokens = None
        output_tokens = None
        if hasattr(message, "usage") and message.usage:
            input_tokens = getattr(message.usage, "input_tokens", None)
            output_tokens = getattr(message.usage, "output_tokens", None)
        request_id = getattr(message, "id", None)
        
        return title, model_name, input_tokens, output_tokens, request_id, latency_ms
        
    except Exception as e:
        print(f"ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å…ˆé ­ã‚’è¿”ã™
        title = first_message[:20] + "..." if len(first_message) > 20 else first_message
        model_name = llm_config.get_model(USE_CASE_TITLE)
        return title, model_name, None, None, None, None


def review_chat(
    system_prompt: str,
    messages: List[Dict[str, str]],
    max_tokens: int = 4096,
    temperature: float = 0.7,
) -> tuple[str, str, Optional[int], Optional[int], Optional[str], Optional[int]]:
    """
    è¬›è©•ãƒãƒ£ãƒƒãƒˆç”¨ã®LLMå‘¼ã³å‡ºã—ï¼ˆthreads/messages ä¿å­˜å‰æï¼‰ã€‚

    - messages ã¯ Anthropic messages å½¢å¼ã®é…åˆ—ï¼ˆrole=user/assistant, contentï¼‰
    - system_prompt ã¯ system ã¨ã—ã¦æ¸¡ã™

    Returns:
        (answer_text, model_name, input_tokens, output_tokens, request_id, latency_ms)
    """
    # LLMè¨­å®šã‚’å–å¾—
    llm_config = get_llm_config()
    
    if not llm_config.is_available():
        return (
            "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ç¾åœ¨LLMæ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ã‚’ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚",
            "dummy",
            None,
            None,
            None,
            None,
        )

    client = llm_config.get_client()
    model_name = llm_config.get_model(USE_CASE_REVIEW_CHAT)
    import time
    start_time = time.time()
    message = client.messages.create(
        model=model_name,
        max_tokens=max_tokens,
        temperature=temperature,
        system=system_prompt or "",
        messages=messages,
    )
    latency_ms = int((time.time() - start_time) * 1000)

    answer = message.content[0].text if message.content and len(message.content) > 0 else ""
    input_tokens = None
    output_tokens = None
    if hasattr(message, "usage") and message.usage:
        input_tokens = getattr(message.usage, "input_tokens", None)
        output_tokens = getattr(message.usage, "output_tokens", None)
    request_id = getattr(message, "id", None)
    return answer, model_name, input_tokens, output_tokens, request_id, latency_ms
